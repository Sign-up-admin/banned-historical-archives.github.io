# æ•°æ®æµå’Œå¤„ç†æµç¨‹æ–‡æ¡£ / Data Flow and Processing Pipeline Document

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°å’Œè°å†å²æ¡£æ¡ˆé¦†é¡¹ç›®ä¸­æ•°æ®ä»åŸå§‹æ–‡ä»¶åˆ°æœ€ç»ˆå±•ç¤ºçš„å®Œæ•´å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†ã€å¤„ç†ã€å­˜å‚¨å’Œè®¿é—®çš„å„ä¸ªç¯èŠ‚ã€‚

## ğŸ“‹ ç›®å½• / Table of Contents

- [æ•°æ®æµæ¦‚è¿° / Data Flow Overview](#æ•°æ®æµæ¦‚è¿°--data-flow-overview)
- [æ•°æ®é‡‡é›†æµç¨‹ / Data Collection Process](#æ•°æ®é‡‡é›†æµç¨‹--data-collection-process)
- [æ•°æ®å¤„ç†æµç¨‹ / Data Processing Pipeline](#æ•°æ®å¤„ç†æµç¨‹--data-processing-pipeline)
- [æ•°æ®å­˜å‚¨æµç¨‹ / Data Storage Process](#æ•°æ®å­˜å‚¨æµç¨‹--data-storage-process)
- [æ•°æ®è®¿é—®æµç¨‹ / Data Access Process](#æ•°æ®è®¿é—®æµç¨‹--data-access-process)
- [æ•°æ®æ›´æ–°æµç¨‹ / Data Update Process](#æ•°æ®æ›´æ–°æµç¨‹--data-update-process)
- [æ•°æ®ç®¡é“æ¶æ„ / Data Pipeline Architecture](#æ•°æ®ç®¡é“æ¶æ„--data-pipeline-architecture)
- [é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ / Error Handling and Retry Mechanism](#é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶--error-handling-and-retry-mechanism)
- [æ•°æ®ç‰ˆæœ¬ç®¡ç† / Data Version Management](#æ•°æ®ç‰ˆæœ¬ç®¡ç†--data-version-management)
- [æ•°æ®è¡€ç¼˜å…³ç³»è¿½è¸ª / Data Lineage Tracking](#æ•°æ®è¡€ç¼˜å…³ç³»è¿½è¸ª--data-lineage-tracking)

## æ•°æ®æµæ¦‚è¿° / Data Flow Overview

### å®Œæ•´æ•°æ®æµå›¾ / Complete Data Flow Diagram

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†é˜¶æ®µ / Data Collection Phase"
        A1[åŸå§‹æ–‡ä»¶<br/>PDF/å›¾ç‰‡] --> A2[æ–‡ä»¶ä¸Šä¼ <br/>GitHub Issue]
        A2 --> A3[è‡ªåŠ¨åŒ–å¤„ç†<br/>GitHub Actions]
    end
    
    subgraph "æ•°æ®å¤„ç†é˜¶æ®µ / Data Processing Phase"
        A3 --> B1[OCRè¯†åˆ«<br/>PaddleOCR]
        B1 --> B2[æ–‡æœ¬è§£æ<br/>ç»“æ„åŒ–å¤„ç†]
        B2 --> B3[è´¨é‡æ£€æŸ¥<br/>æ ¼å¼éªŒè¯]
        B3 --> B4[äººå·¥æ ¡å¯¹<br/>è¡¥ä¸åº”ç”¨]
    end
    
    subgraph "æ•°æ®å­˜å‚¨é˜¶æ®µ / Data Storage Phase"
        B4 --> C1[èµ„æºä»“åº“<br/>parsedåˆ†æ”¯]
        C1 --> C2[ä¸»ä»“åº“<br/>æ„å»ºè„šæœ¬]
        C2 --> C3[ç´¢å¼•æ„å»º<br/>build-indexes]
        C2 --> C4[JSONæ„å»º<br/>build-article-json]
    end
    
    subgraph "æ•°æ®è®¿é—®é˜¶æ®µ / Data Access Phase"
        C3 --> D1[indexesåˆ†æ”¯<br/>ç´¢å¼•æ•°æ®]
        C4 --> D2[jsonåˆ†æ”¯<br/>æ–‡ç« æ•°æ®]
        D1 --> D3[å‰ç«¯åº”ç”¨<br/>æ•°æ®åŠ è½½]
        D2 --> D3
        D3 --> D4[ç”¨æˆ·ç•Œé¢<br/>å†…å®¹å±•ç¤º]
    end
    
    style A1 fill:#e1f5ff
    style B1 fill:#fff4e1
    style C1 fill:#e8f5e9
    style D4 fill:#f3e5f5
```

### æ•°æ®æµå…³é”®èŠ‚ç‚¹ / Key Data Flow Nodes

| é˜¶æ®µ | è¾“å…¥ | å¤„ç† | è¾“å‡º |
|------|------|------|------|
| é‡‡é›† | åŸå§‹PDF/å›¾ç‰‡ | æ–‡ä»¶ä¸Šä¼  | èµ„æºä»“åº“mainåˆ†æ”¯ |
| OCR | PDF/å›¾ç‰‡ | PaddleOCRè¯†åˆ« | ocr_cacheåˆ†æ”¯ |
| è§£æ | OCRç»“æœ | ç»“æ„åŒ–è§£æ | parsedåˆ†æ”¯ |
| æ ¡å¯¹ | parsedæ•°æ® | äººå·¥ä¿®æ­£ | ocr_patchåˆ†æ”¯ |
| æ„å»º | parsedæ•°æ® | ç´¢å¼•/JSONæ„å»º | indexes/jsonåˆ†æ”¯ |
| è®¿é—® | indexes/json | å‰ç«¯åŠ è½½ | ç”¨æˆ·ç•Œé¢ |

## æ•°æ®é‡‡é›†æµç¨‹ / Data Collection Process

### æ–‡ä»¶ä¸Šä¼ æµç¨‹ / File Upload Process

```mermaid
sequenceDiagram
    participant User as è´¡çŒ®è€…
    participant Issue as GitHub Issue
    participant Action as GitHub Actions
    participant Repo as èµ„æºä»“åº“
    
    User->>Issue: åˆ›å»ºå½•å…¥Issue
    User->>Issue: ä¸Šä¼ åŸå§‹æ–‡ä»¶
    User->>Issue: å¡«å†™é…ç½®ä¿¡æ¯
    
    Issue->>Action: è§¦å‘è‡ªåŠ¨åŒ–å·¥ä½œæµ
    Action->>Action: ä¸‹è½½æ–‡ä»¶
    Action->>Action: éªŒè¯æ–‡ä»¶æ ¼å¼
    Action->>Repo: æ¨é€åˆ°mainåˆ†æ”¯
    Action->>Repo: åˆ›å»ºconfigåˆ†æ”¯PR
    Repo-->>User: é€šçŸ¥å¤„ç†å®Œæˆ
```

### æ–‡ä»¶æ ¼å¼è¦æ±‚ / File Format Requirements

#### æ”¯æŒçš„æ ¼å¼

| æ ¼å¼ | æ‰©å±•å | ç”¨é€” | å¤„ç†æ–¹å¼ |
|------|--------|------|---------|
| PDF | `.pdf` | æ–‡æ¡£æ–‡ä»¶ | PDFè§£æ + OCR |
| å›¾ç‰‡ | `.png`, `.jpg`, `.jpeg` | æ‰«æå›¾ç‰‡ | ç›´æ¥OCR |
| EPUB | `.epub` | ç”µå­ä¹¦ | EPUBè§£æ |

#### æ–‡ä»¶è´¨é‡è¦æ±‚

1. **åˆ†è¾¨ç‡**: è‡³å°‘ 200 DPIï¼Œæ¨è 300+ DPI
2. **æ ¼å¼**: æ¸…æ™°çš„æ‰«æç‰ˆï¼Œé¿å…äºŒæ¬¡æ’ç‰ˆ
3. **å®Œæ•´æ€§**: æ–‡ä»¶å®Œæ•´ï¼Œæ— ç¼ºå¤±é¡µé¢
4. **æ¥æº**: æ³¨æ˜æ–‡ä»¶æ¥æºï¼Œç¡®ä¿å¯è¿½æº¯

### é…ç½®ä¿¡æ¯ç»“æ„ / Configuration Structure

```typescript
interface UploadConfig {
  source_name: string;        // æ¥æºåç§°
  archive_id: number;        // èµ„æºä»“åº“ID
  internal: boolean;          // æ˜¯å¦å†…éƒ¨æ–‡ä»¶
  official: boolean;          // æ˜¯å¦å®˜æ–¹æ–‡ä»¶
  author: string;             // ä½œè€…ä¿¡æ¯
  articles: ArticleConfig[]; // æ–‡ç« é…ç½®åˆ—è¡¨
  ocr?: OCRConfig;            // OCRé…ç½®ï¼ˆå¯é€‰ï¼‰
}

interface ArticleConfig {
  title: string;             // æ–‡ç« æ ‡é¢˜
  authors: string[];         // ä½œè€…åˆ—è¡¨
  dates: DateObject[];      // æ—¥æœŸåˆ—è¡¨
  is_range_date: boolean;    // æ˜¯å¦æ—¥æœŸèŒƒå›´
  page_start: number;        // èµ·å§‹é¡µç 
  page_end: number;          // ç»“æŸé¡µç 
  alias?: string;            // åˆ«å
}
```

## æ•°æ®å¤„ç†æµç¨‹ / Data Processing Pipeline

### OCRå¤„ç†æµç¨‹ / OCR Processing Pipeline

```mermaid
graph LR
    A[åŸå§‹æ–‡ä»¶] --> B[PDFè§£æ<br/>æå–é¡µé¢]
    B --> C[å›¾ç‰‡é¢„å¤„ç†<br/>ç°åº¦åŒ–/äºŒå€¼åŒ–]
    C --> D[OCRè¯†åˆ«<br/>PaddleOCR]
    D --> E[æ–‡æœ¬åå¤„ç†<br/>æ¸…ç†/æ ¼å¼åŒ–]
    E --> F[ç»“æ„åŒ–è§£æ<br/>æ®µè½/æ ‡é¢˜è¯†åˆ«]
    F --> G[è´¨é‡æ£€æŸ¥<br/>æ ¼å¼éªŒè¯]
    G --> H{è´¨é‡åˆæ ¼?}
    H -->|æ˜¯| I[ä¿å­˜åˆ°ocr_cache]
    H -->|å¦| J[äººå·¥å®¡æ ¸]
    J --> K[ç”Ÿæˆè¡¥ä¸]
    K --> L[ä¿å­˜åˆ°ocr_patch]
```

### OCRå¤„ç†è¯¦ç»†æ­¥éª¤ / Detailed OCR Processing Steps

#### 1. æ–‡ä»¶é¢„å¤„ç†

```typescript
// æ–‡ä»¶ç±»å‹æ£€æµ‹å’Œé¢„å¤„ç†
async function preprocessFile(filePath: string): Promise<ProcessedFile> {
  const fileType = await detectFileType(filePath);
  
  switch (fileType) {
    case 'pdf':
      return await extractPagesFromPDF(filePath);
    case 'image':
      return await loadImage(filePath);
    case 'epub':
      return await extractTextFromEPUB(filePath);
    default:
      throw new Error(`ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: ${fileType}`);
  }
}
```

#### 2. OCRè¯†åˆ«

```typescript
// PaddleOCRè¯†åˆ«é…ç½®
interface OCRConfig {
  rec_model: string;              // è¯†åˆ«æ¨¡å‹
  det_model: string;              // æ£€æµ‹æ¨¡å‹
  content_thresholds: number[];    // å†…å®¹é˜ˆå€¼
  use_angle_cls: boolean;          // ä½¿ç”¨è§’åº¦åˆ†ç±»
}

// OCRè¯†åˆ«æµç¨‹
async function performOCR(image: Image, config: OCRConfig): Promise<OCRResult> {
  // 1. æ–‡æœ¬æ£€æµ‹
  const textBoxes = await detectText(image, config.det_model);
  
  // 2. æ–‡æœ¬è¯†åˆ«
  const texts = await Promise.all(
    textBoxes.map(box => recognizeText(box, config.rec_model))
  );
  
  // 3. ç»“æœåˆå¹¶
  return mergeOCRResults(textBoxes, texts);
}
```

#### 3. æ–‡æœ¬ç»“æ„åŒ–è§£æ

```typescript
// è§£æOCRç»“æœï¼Œè¯†åˆ«æ®µè½ã€æ ‡é¢˜ç­‰ç»“æ„
function parseOCRResult(ocrResult: OCRResult): ParserResult {
  const parts: ContentPart[] = [];
  let currentParagraph = '';
  
  for (const line of ocrResult.lines) {
    // è¯†åˆ«æ ‡é¢˜ï¼ˆé€šå¸¸å­—ä½“è¾ƒå¤§ã€å±…ä¸­ï¼‰
    if (isTitle(line)) {
      if (currentParagraph) {
        parts.push({ type: 'paragraph', text: currentParagraph });
        currentParagraph = '';
      }
      parts.push({ type: 'title', text: line.text });
    }
    // è¯†åˆ«ä½œè€…è¡Œ
    else if (isAuthorLine(line)) {
      if (currentParagraph) {
        parts.push({ type: 'paragraph', text: currentParagraph });
        currentParagraph = '';
      }
      parts.push({ type: 'authors', text: line.text });
    }
    // æ™®é€šæ®µè½
    else {
      currentParagraph += line.text + '\n';
    }
  }
  
  // æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
  if (currentParagraph) {
    parts.push({ type: 'paragraph', text: currentParagraph });
  }
  
  return {
    title: extractTitle(parts),
    authors: extractAuthors(parts),
    dates: extractDates(parts),
    parts: parts,
    // ... å…¶ä»–å­—æ®µ
  };
}
```

### è´¨é‡æ£€æŸ¥å’ŒéªŒè¯ / Quality Check and Validation

#### æ•°æ®æ ¼å¼éªŒè¯

```typescript
// éªŒè¯è§£æç»“æœæ ¼å¼
function validateParserResult(result: ParserResult): ValidationResult {
  const errors: string[] = [];
  
  // æ£€æŸ¥å¿…éœ€å­—æ®µ
  if (!result.title) {
    errors.push('ç¼ºå°‘æ ‡é¢˜');
  }
  
  if (!result.authors || result.authors.length === 0) {
    errors.push('ç¼ºå°‘ä½œè€…ä¿¡æ¯');
  }
  
  if (!result.dates || result.dates.length === 0) {
    errors.push('ç¼ºå°‘æ—¥æœŸä¿¡æ¯');
  }
  
  // æ£€æŸ¥å†…å®¹æ®µè½
  if (!result.parts || result.parts.length === 0) {
    errors.push('ç¼ºå°‘å†…å®¹æ®µè½');
  }
  
  // æ£€æŸ¥æ®µè½ç±»å‹
  const validTypes = ['title', 'paragraph', 'authors', 'subtitle'];
  for (const part of result.parts) {
    if (!validTypes.includes(part.type)) {
      errors.push(`æ— æ•ˆçš„æ®µè½ç±»å‹: ${part.type}`);
    }
  }
  
  return {
    isValid: errors.length === 0,
    errors,
  };
}
```

#### è´¨é‡è¯„åˆ†

```typescript
// è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°
function calculateQualityScore(result: ParserResult): number {
  let score = 100;
  
  // æ ‡é¢˜è´¨é‡ (-10 if missing)
  if (!result.title) score -= 10;
  
  // ä½œè€…ä¿¡æ¯ (-5 if missing)
  if (!result.authors || result.authors.length === 0) score -= 5;
  
  // æ—¥æœŸä¿¡æ¯ (-5 if missing)
  if (!result.dates || result.dates.length === 0) score -= 5;
  
  // å†…å®¹å®Œæ•´æ€§ (-20 if too short)
  const totalLength = result.parts.reduce((sum, p) => sum + p.text.length, 0);
  if (totalLength < 100) score -= 20;
  
  // æ ¼å¼è§„èŒƒæ€§ (-10 if invalid format)
  if (!validateFormat(result)) score -= 10;
  
  return Math.max(0, score);
}
```

## æ•°æ®å­˜å‚¨æµç¨‹ / Data Storage Process

### å­˜å‚¨ç»“æ„ / Storage Structure

```
èµ„æºä»“åº“å­˜å‚¨ç»“æ„:
archives{id}/
â”œâ”€â”€ main/                    # åŸå§‹æ–‡ä»¶åˆ†æ”¯
â”‚   â””â”€â”€ {resource_id}/
â”‚       â”œâ”€â”€ file.pdf         # PDFæ–‡ä»¶
â”‚       â””â”€â”€ images/          # å›¾ç‰‡æ–‡ä»¶
â”‚           â”œâ”€â”€ 001.png
â”‚           â””â”€â”€ 002.png
â”‚
â”œâ”€â”€ config/                  # é…ç½®åˆ†æ”¯
â”‚   â””â”€â”€ {resource_id}.ts    # TypeScripté…ç½®
â”‚
â”œâ”€â”€ ocr_cache/               # OCRç¼“å­˜åˆ†æ”¯
â”‚   â””â”€â”€ {resource_id}/
â”‚       â””â”€â”€ {article_id}.json
â”‚
â”œâ”€â”€ ocr_patch/               # OCRè¡¥ä¸åˆ†æ”¯
â”‚   â””â”€â”€ {resource_id}/
â”‚       â””â”€â”€ {article_id}.json
â”‚
â””â”€â”€ parsed/                  # è§£ææ•°æ®åˆ†æ”¯
    â””â”€â”€ {prefix}/            # IDå‰3ä½
        â””â”€â”€ {article_id}/
            â”œâ”€â”€ {article_id}.json      # è§£æç»“æœ
            â”œâ”€â”€ {article_id}.tags      # æ ‡ç­¾æ•°æ®
            â””â”€â”€ {article_id}.metadata  # å…ƒæ•°æ®
```

### æ•°æ®å­˜å‚¨æµç¨‹ / Data Storage Flow

```mermaid
sequenceDiagram
    participant OCR as OCRå¤„ç†
    participant Cache as ocr_cacheåˆ†æ”¯
    participant Patch as ocr_patchåˆ†æ”¯
    participant Parsed as parsedåˆ†æ”¯
    participant Build as æ„å»ºè„šæœ¬
    participant Index as indexesåˆ†æ”¯
    participant JSON as jsonåˆ†æ”¯
    
    OCR->>Cache: ä¿å­˜OCRç»“æœ
    OCR->>Parsed: åº”ç”¨è¡¥ä¸ç”Ÿæˆè§£ææ•°æ®
    
    alt éœ€è¦äººå·¥æ ¡å¯¹
        Cache->>Patch: ç”Ÿæˆè¡¥ä¸æ–‡ä»¶
        Patch->>Parsed: åº”ç”¨è¡¥ä¸
    end
    
    Parsed->>Build: è§¦å‘æ„å»º
    Build->>Index: æ„å»ºç´¢å¼•æ•°æ®
    Build->>JSON: æ„å»ºJSONæ•°æ®
```

## æ•°æ®è®¿é—®æµç¨‹ / Data Access Process

### å‰ç«¯æ•°æ®åŠ è½½æµç¨‹ / Frontend Data Loading Flow

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Frontend as å‰ç«¯åº”ç”¨
    participant API as GitHub Raw API
    participant Index as indexesåˆ†æ”¯
    participant JSON as jsonåˆ†æ”¯
    participant ES as Elasticsearch
    
    User->>Frontend: è®¿é—®æ–‡ç« åˆ—è¡¨é¡µ
    Frontend->>API: è¯·æ±‚file_count.json
    API->>Index: è·å–æ–‡ä»¶è®¡æ•°
    Index-->>API: è¿”å›è®¡æ•°
    API-->>Frontend: è¿”å›æ•°æ®
    
    Frontend->>API: è¯·æ±‚article_list_0.json
    API->>Index: è·å–æ–‡ç« åˆ—è¡¨ç´¢å¼•
    Index-->>API: è¿”å›ç´¢å¼•æ•°æ®
    API-->>Frontend: è¿”å›æ•°æ®
    
    Frontend->>Frontend: å®¢æˆ·ç«¯ç­›é€‰/æ’åº
    Frontend-->>User: æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨
    
    User->>Frontend: ç‚¹å‡»æ–‡ç« 
    Frontend->>API: è¯·æ±‚æ–‡ç« JSON
    API->>JSON: è·å–æ–‡ç« æ•°æ®
    JSON-->>API: è¿”å›å®Œæ•´æ•°æ®
    API-->>Frontend: è¿”å›æ•°æ®
    Frontend-->>User: æ˜¾ç¤ºæ–‡ç« è¯¦æƒ…
```

### æœç´¢æ•°æ®æµç¨‹ / Search Data Flow

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Frontend as å‰ç«¯åº”ç”¨
    participant ES as Elasticsearch
    participant API as GitHub Raw API
    participant JSON as jsonåˆ†æ”¯
    
    User->>Frontend: è¾“å…¥æœç´¢å…³é”®è¯
    Frontend->>ES: å‘é€æœç´¢è¯·æ±‚
    ES->>ES: æ‰§è¡Œå…¨æ–‡æœç´¢
    ES-->>Frontend: è¿”å›æœç´¢ç»“æœIDåˆ—è¡¨
    
    Frontend->>Frontend: å¤„ç†æœç´¢ç»“æœ
    Frontend->>API: æ‰¹é‡è¯·æ±‚æ–‡ç« æ•°æ®
    API->>JSON: è·å–æ–‡ç« æ•°æ®
    JSON-->>API: è¿”å›æ•°æ®
    API-->>Frontend: è¿”å›æ•°æ®
    Frontend-->>User: æ˜¾ç¤ºæœç´¢ç»“æœ
```

### æ•°æ®ç¼“å­˜ç­–ç•¥ / Data Caching Strategy

#### æµè§ˆå™¨ç¼“å­˜

```typescript
// ä½¿ç”¨Cache APIç¼“å­˜æ•°æ®
async function getCachedData<T>(key: string): Promise<T | null> {
  const cache = await caches.open('data-cache-v1');
  const cached = await cache.match(key);
  
  if (cached) {
    const data = await cached.json();
    // æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ5åˆ†é’Ÿï¼‰
    const age = Date.now() - data.timestamp;
    if (age < 5 * 60 * 1000) {
      return data.value;
    }
  }
  
  return null;
}

async function setCachedData<T>(key: string, value: T): Promise<void> {
  const cache = await caches.open('data-cache-v1');
  const response = new Response(JSON.stringify({
    value,
    timestamp: Date.now(),
  }));
  await cache.put(key, response);
}
```

#### å†…å­˜ç¼“å­˜

```typescript
// ä½¿ç”¨Mapå®ç°å†…å­˜ç¼“å­˜
class MemoryCache {
  private cache = new Map<string, { data: any; timestamp: number }>();
  private readonly TTL = 5 * 60 * 1000; // 5åˆ†é’Ÿ

  get<T>(key: string): T | null {
    const cached = this.cache.get(key);
    if (cached && Date.now() - cached.timestamp < this.TTL) {
      return cached.data as T;
    }
    return null;
  }

  set<T>(key: string, data: T): void {
    this.cache.set(key, { data, timestamp: Date.now() });
  }

  clear(): void {
    this.cache.clear();
  }
}
```

## æ•°æ®æ›´æ–°æµç¨‹ / Data Update Process

### æ•°æ®æ›´æ–°è§¦å‘ / Data Update Trigger

```mermaid
graph LR
    A[æ•°æ®å˜æ›´] --> B{å˜æ›´ç±»å‹}
    B -->|æ–°æ–‡ä»¶| C[è§¦å‘OCRå¤„ç†]
    B -->|è¡¥ä¸æ›´æ–°| D[åº”ç”¨è¡¥ä¸]
    B -->|é…ç½®å˜æ›´| E[é‡æ–°è§£æ]
    
    C --> F[æ›´æ–°parsedåˆ†æ”¯]
    D --> F
    E --> F
    
    F --> G[è§¦å‘æ„å»º]
    G --> H[æ›´æ–°indexesåˆ†æ”¯]
    G --> I[æ›´æ–°jsonåˆ†æ”¯]
    
    H --> J[å‰ç«¯è‡ªåŠ¨æ›´æ–°]
    I --> J
```

### å¢é‡æ›´æ–°æœºåˆ¶ / Incremental Update Mechanism

```typescript
// æ£€æµ‹æ•°æ®å˜æ›´
async function detectChanges(): Promise<string[]> {
  const lastBuildTime = await getLastBuildTime();
  const changedFiles: string[] = [];
  
  // æ‰«æparsedç›®å½•
  const files = await scanParsedDirectory();
  for (const file of files) {
    const stats = await fs.stat(file);
    if (stats.mtimeMs > lastBuildTime) {
      changedFiles.push(file);
    }
  }
  
  return changedFiles;
}

// å¢é‡æ„å»º
async function incrementalBuild() {
  const changedFiles = await detectChanges();
  
  if (changedFiles.length === 0) {
    console.log('æ²¡æœ‰å˜æ›´ï¼Œè·³è¿‡æ„å»º');
    return;
  }
  
  // åªæ„å»ºå˜æ›´çš„æ–‡ä»¶
  for (const file of changedFiles) {
    await buildArticle(file);
  }
  
  // æ›´æ–°ç´¢å¼•
  await updateIndexes(changedFiles);
}
```

### ç‰ˆæœ¬æ§åˆ¶ / Version Control

#### Gitç‰ˆæœ¬ç®¡ç†

- **æ•°æ®ç‰ˆæœ¬**: é€šè¿‡Gitæäº¤å†å²è¿½è¸ªæ•°æ®å˜æ›´
- **è¡¥ä¸ç‰ˆæœ¬**: ocr_patchåˆ†æ”¯è®°å½•æ¯æ¬¡ä¿®æ­£
- **æ„å»ºç‰ˆæœ¬**: æ¯æ¬¡æ„å»ºç”Ÿæˆæ–°çš„ç‰ˆæœ¬æ ‡ç­¾

#### æ•°æ®æº¯æº

```typescript
// è¿½è¸ªæ•°æ®æ¥æº
interface DataProvenance {
  source_file: string;        // åŸå§‹æ–‡ä»¶
  ocr_version: string;         // OCRç‰ˆæœ¬
  patch_version?: string;      // è¡¥ä¸ç‰ˆæœ¬
  build_time: string;          // æ„å»ºæ—¶é—´
  build_commit: string;        // æ„å»ºæäº¤
}

// ç”Ÿæˆæº¯æºä¿¡æ¯
function generateProvenance(articleId: string): DataProvenance {
  return {
    source_file: getSourceFile(articleId),
    ocr_version: getOCRVersion(articleId),
    patch_version: getPatchVersion(articleId),
    build_time: new Date().toISOString(),
    build_commit: getCurrentCommit(),
  };
}
```

## æ•°æ®ç®¡é“æ¶æ„ / Data Pipeline Architecture

### å®Œæ•´æ•°æ®ç®¡é“å›¾ / Complete Data Pipeline Diagram

```mermaid
graph TB
    subgraph "æ•°æ®æºå±‚ / Data Source Layer"
        DS1[èµ„æºä»“åº“0-31<br/>GitHub Repositories]
        DS2[mainåˆ†æ”¯<br/>åŸå§‹æ–‡ä»¶]
        DS3[configåˆ†æ”¯<br/>é…ç½®æ–‡ä»¶]
        DS4[parsedåˆ†æ”¯<br/>è§£ææ•°æ®]
        DS5[ocr_cacheåˆ†æ”¯<br/>OCRç¼“å­˜]
        DS6[ocr_patchåˆ†æ”¯<br/>OCRè¡¥ä¸]
    end
    
    subgraph "æ•°æ®æå–å±‚ / Extract Layer"
        E1[init-sub-repository.ts<br/>ä¸‹è½½å­ä»“åº“æ•°æ®]
        E2[æ•°æ®éªŒè¯<br/>æ ¼å¼æ£€æŸ¥]
        E3[æ•°æ®åˆå¹¶<br/>å¤šæºæ•´åˆ]
    end
    
    subgraph "æ•°æ®è½¬æ¢å±‚ / Transform Layer"
        T1[æ•°æ®æ¸…æ´—<br/>å»é™¤æ— æ•ˆæ•°æ®]
        T2[æ•°æ®æ ‡å‡†åŒ–<br/>æ ¼å¼ç»Ÿä¸€]
        T3[æ•°æ®éªŒè¯<br/>è´¨é‡æ£€æŸ¥]
        T4[è¡¥ä¸åº”ç”¨<br/>äººå·¥ä¿®æ­£]
        T5[æ ‡ç­¾æå–<br/>å…ƒæ•°æ®ç”Ÿæˆ]
    end
    
    subgraph "æ•°æ®åŠ è½½å±‚ / Load Layer"
        L1[build-indexes.ts<br/>æ„å»ºç´¢å¼•]
        L2[build-article-json.ts<br/>æ„å»ºJSON]
        L3[build-sitemap.ts<br/>æ„å»ºç½‘ç«™åœ°å›¾]
        L4[build-txt.ts<br/>æ„å»ºæ–‡æœ¬å¯¼å‡º]
        L5[init-es.ts<br/>åˆå§‹åŒ–æœç´¢ç´¢å¼•]
    end
    
    subgraph "æ•°æ®è¾“å‡ºå±‚ / Output Layer"
        O1[indexesåˆ†æ”¯<br/>ç´¢å¼•æ•°æ®]
        O2[jsonåˆ†æ”¯<br/>æ–‡ç« æ•°æ®]
        O3[txtåˆ†æ”¯<br/>æ–‡æœ¬æ•°æ®]
        O4[gh-pagesåˆ†æ”¯<br/>é™æ€ç½‘ç«™]
        O5[Elasticsearch<br/>æœç´¢ç´¢å¼•]
    end
    
    DS1 --> E1
    DS2 --> E1
    DS3 --> E1
    DS4 --> E1
    DS5 --> E1
    DS6 --> E1
    
    E1 --> E2
    E2 --> E3
    
    E3 --> T1
    T1 --> T2
    T2 --> T3
    T3 --> T4
    T4 --> T5
    
    T5 --> L1
    T5 --> L2
    T5 --> L3
    T5 --> L4
    T5 --> L5
    
    L1 --> O1
    L2 --> O2
    L3 --> O4
    L4 --> O3
    L5 --> O5
    
    style DS1 fill:#e1f5ff
    style E1 fill:#fff4e1
    style T1 fill:#e8f5e9
    style L1 fill:#f3e5f5
    style O1 fill:#fce4ec
```

### æ•°æ®ç®¡é“æ‰§è¡Œé¡ºåº / Pipeline Execution Order

```mermaid
sequenceDiagram
    participant CI as CI/CDç³»ç»Ÿ
    participant Init as init-sub-repository
    participant Index as build-indexes
    participant JSON as build-article-json
    participant Sitemap as build-sitemap
    participant Txt as build-txt
    participant ES as init-es
    
    CI->>Init: 1. ä¸‹è½½æœ€æ–°æ•°æ®
    Init-->>CI: æ•°æ®ä¸‹è½½å®Œæˆ
    
    CI->>Index: 2. æ„å»ºç´¢å¼•
    Index->>Index: æ‰«æparsedç›®å½•
    Index->>Index: æ„å»ºæ–‡ç« ç´¢å¼•
    Index->>Index: æ„å»ºæ ‡ç­¾ç´¢å¼•
    Index->>Index: æ„å»ºä¹¦ç±ç´¢å¼•
    Index-->>CI: ç´¢å¼•æ„å»ºå®Œæˆ
    
    CI->>JSON: 3. æ„å»ºJSONæ•°æ®
    JSON->>JSON: è¯»å–ç´¢å¼•
    JSON->>JSON: åˆå¹¶æ–‡ç« æ•°æ®
    JSON->>JSON: ç”ŸæˆJSONæ–‡ä»¶
    JSON-->>CI: JSONæ„å»ºå®Œæˆ
    
    CI->>Txt: 4. æ„å»ºæ–‡æœ¬å¯¼å‡º
    Txt->>Txt: è¯»å–æ–‡ç« æ•°æ®
    Txt->>Txt: æ ¼å¼åŒ–æ–‡æœ¬
    Txt->>Txt: ç”ŸæˆTXTæ–‡ä»¶
    Txt-->>CI: æ–‡æœ¬æ„å»ºå®Œæˆ
    
    CI->>ES: 5. åˆå§‹åŒ–æœç´¢ç´¢å¼•
    ES->>ES: è¯»å–æ–‡ç« æ•°æ®
    ES->>ES: ç´¢å¼•åˆ°Elasticsearch
    ES-->>CI: æœç´¢ç´¢å¼•å®Œæˆ
    
    CI->>Sitemap: 6. æ„å»ºç½‘ç«™åœ°å›¾
    Sitemap->>Sitemap: ç”Ÿæˆsitemap.xml
    Sitemap-->>CI: ç½‘ç«™åœ°å›¾å®Œæˆ
```

### æ•°æ®è½¬æ¢æ­¥éª¤è¯¦è§£ / Detailed Data Transformation Steps

#### æ­¥éª¤1: æ•°æ®æå– (Extract)

```typescript
// æ•°æ®æå–æµç¨‹
interface ExtractResult {
  parsed: ParsedData[];      // è§£æåçš„æ•°æ®
  config: ConfigData[];       // é…ç½®æ•°æ®
  metadata: Metadata[];       // å…ƒæ•°æ®
  patches: PatchData[];       // è¡¥ä¸æ•°æ®
}

async function extractData(): Promise<ExtractResult> {
  const result: ExtractResult = {
    parsed: [],
    config: [],
    metadata: [],
    patches: [],
  };
  
  // éå†æ‰€æœ‰èµ„æºä»“åº“ (0-31)
  for (let i = 0; i <= 31; i++) {
    const archivePath = `parsed/archives${i}`;
    
    // æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨
    if (!await fs.pathExists(archivePath)) {
      continue;
    }
    
    // è¯»å–parsedæ•°æ®
    const parsedData = await readParsedData(archivePath);
    result.parsed.push(...parsedData);
    
    // è¯»å–configæ•°æ®
    const configData = await readConfigData(`config/archives${i}`);
    result.config.push(...configData);
    
    // è¯»å–è¡¥ä¸æ•°æ®
    const patchData = await readPatchData(`ocr_patch/archives${i}`);
    result.patches.push(...patchData);
  }
  
  return result;
}
```

#### æ­¥éª¤2: æ•°æ®è½¬æ¢ (Transform)

```typescript
// æ•°æ®è½¬æ¢æµç¨‹
interface TransformResult {
  articles: Article[];        // æ ‡å‡†åŒ–æ–‡ç« 
  indexes: IndexData;         // ç´¢å¼•æ•°æ®
  errors: ValidationError[];  // éªŒè¯é”™è¯¯
}

async function transformData(extracted: ExtractResult): Promise<TransformResult> {
  const articles: Article[] = [];
  const errors: ValidationError[] = [];
  
  // åº”ç”¨è¡¥ä¸
  for (const patch of extracted.patches) {
    const article = findArticle(extracted.parsed, patch.articleId);
    if (article) {
      applyPatch(article, patch);
    }
  }
  
  // æ•°æ®æ ‡å‡†åŒ–
  for (const parsed of extracted.parsed) {
    try {
      // éªŒè¯æ•°æ®æ ¼å¼
      const validation = validateParserResult(parsed);
      if (!validation.isValid) {
        errors.push(...validation.errors);
        continue;
      }
      
      // æ ‡å‡†åŒ–å¤„ç†
      const article = standardizeArticle(parsed, extracted.config);
      articles.push(article);
    } catch (error) {
      errors.push({
        articleId: parsed.id,
        error: error.message,
      });
    }
  }
  
  // æ„å»ºç´¢å¼•
  const indexes = buildIndexes(articles);
  
  return {
    articles,
    indexes,
    errors,
  };
}
```

#### æ­¥éª¤3: æ•°æ®åŠ è½½ (Load)

```typescript
// æ•°æ®åŠ è½½æµç¨‹
async function loadData(transformed: TransformResult): Promise<void> {
  // 1. æ„å»ºç´¢å¼•æ–‡ä»¶
  await buildIndexFiles(transformed.indexes);
  
  // 2. æ„å»ºJSONæ–‡ä»¶
  await buildJSONFiles(transformed.articles);
  
  // 3. æ„å»ºæ–‡æœ¬æ–‡ä»¶
  await buildTextFiles(transformed.articles);
  
  // 4. æ›´æ–°æœç´¢ç´¢å¼•
  await updateSearchIndex(transformed.articles);
  
  // 5. æ„å»ºç½‘ç«™åœ°å›¾
  await buildSitemap(transformed.articles);
  
  // 6. æŠ¥å‘Šé”™è¯¯
  if (transformed.errors.length > 0) {
    await reportErrors(transformed.errors);
  }
}
```

## é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ / Error Handling and Retry Mechanism

### é”™è¯¯åˆ†ç±» / Error Classification

```typescript
enum ErrorType {
  NETWORK_ERROR = 'NETWORK_ERROR',           // ç½‘ç»œé”™è¯¯
  FILE_NOT_FOUND = 'FILE_NOT_FOUND',         // æ–‡ä»¶ä¸å­˜åœ¨
  PARSE_ERROR = 'PARSE_ERROR',               // è§£æé”™è¯¯
  VALIDATION_ERROR = 'VALIDATION_ERROR',     // éªŒè¯é”™è¯¯
  BUILD_ERROR = 'BUILD_ERROR',               // æ„å»ºé”™è¯¯
  UNKNOWN_ERROR = 'UNKNOWN_ERROR',           // æœªçŸ¥é”™è¯¯
}

interface ErrorInfo {
  type: ErrorType;
  message: string;
  articleId?: string;
  filePath?: string;
  timestamp: Date;
  retryCount: number;
  maxRetries: number;
}
```

### é‡è¯•ç­–ç•¥ / Retry Strategy

```typescript
// æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  baseDelay: number = 1000
): Promise<T> {
  let lastError: Error;
  
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      
      // å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
      if (attempt === maxRetries - 1) {
        throw error;
      }
      
      // è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
      const delay = baseDelay * Math.pow(2, attempt);
      console.warn(`é‡è¯• ${attempt + 1}/${maxRetries}ï¼Œ${delay}msåé‡è¯•...`);
      
      await sleep(delay);
    }
  }
  
  throw lastError!;
}

// ä½¿ç”¨ç¤ºä¾‹
async function downloadWithRetry(url: string): Promise<Buffer> {
  return retryWithBackoff(async () => {
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }
    return Buffer.from(await response.arrayBuffer());
  }, 3, 1000);
}
```

### é”™è¯¯æ¢å¤æœºåˆ¶ / Error Recovery Mechanism

```typescript
// é”™è¯¯æ¢å¤ç­–ç•¥
class ErrorRecovery {
  private errorQueue: ErrorInfo[] = [];
  
  // è®°å½•é”™è¯¯
  recordError(error: ErrorInfo): void {
    this.errorQueue.push(error);
    
    // æ ¹æ®é”™è¯¯ç±»å‹é€‰æ‹©æ¢å¤ç­–ç•¥
    switch (error.type) {
      case ErrorType.NETWORK_ERROR:
        this.scheduleRetry(error);
        break;
      case ErrorType.FILE_NOT_FOUND:
        this.reportMissingFile(error);
        break;
      case ErrorType.PARSE_ERROR:
        this.markForManualReview(error);
        break;
      case ErrorType.VALIDATION_ERROR:
        this.logValidationError(error);
        break;
      default:
        this.reportUnknownError(error);
    }
  }
  
  // è®¡åˆ’é‡è¯•
  private scheduleRetry(error: ErrorInfo): void {
    if (error.retryCount < error.maxRetries) {
      setTimeout(() => {
        this.retryOperation(error);
      }, 1000 * Math.pow(2, error.retryCount));
    }
  }
  
  // æ ‡è®°éœ€è¦äººå·¥å®¡æ ¸
  private markForManualReview(error: ErrorInfo): void {
    // åˆ›å»ºIssueæˆ–é€šçŸ¥ç®¡ç†å‘˜
    console.warn(`éœ€è¦äººå·¥å®¡æ ¸: ${error.articleId}`);
  }
  
  // æŠ¥å‘Šç¼ºå¤±æ–‡ä»¶
  private reportMissingFile(error: ErrorInfo): void {
    console.error(`æ–‡ä»¶ä¸å­˜åœ¨: ${error.filePath}`);
    // å¯ä»¥å°è¯•ä»å¤‡ä»½æ¢å¤æˆ–è·³è¿‡
  }
}
```

### é”™è¯¯ç›‘æ§å’Œå‘Šè­¦ / Error Monitoring and Alerting

```typescript
// é”™è¯¯ç›‘æ§
class ErrorMonitor {
  private errorCounts = new Map<ErrorType, number>();
  private errorThresholds = new Map<ErrorType, number>([
    [ErrorType.NETWORK_ERROR, 10],
    [ErrorType.PARSE_ERROR, 5],
    [ErrorType.VALIDATION_ERROR, 20],
  ]);
  
  // æ£€æŸ¥é”™è¯¯é˜ˆå€¼
  checkThresholds(): void {
    for (const [type, threshold] of this.errorThresholds) {
      const count = this.errorCounts.get(type) || 0;
      if (count >= threshold) {
        this.sendAlert(type, count);
      }
    }
  }
  
  // å‘é€å‘Šè­¦
  private sendAlert(type: ErrorType, count: number): void {
    console.error(`å‘Šè­¦: ${type} é”™è¯¯æ•°é‡è¾¾åˆ° ${count}`);
    // å‘é€é‚®ä»¶ã€Slacké€šçŸ¥ç­‰
  }
}
```

## æ•°æ®ç‰ˆæœ¬ç®¡ç† / Data Version Management

### ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥ / Version Control Strategy

```typescript
interface DataVersion {
  version: string;              // ç‰ˆæœ¬å· (semveræ ¼å¼)
  commitHash: string;           // Gitæäº¤å“ˆå¸Œ
  buildTime: string;            // æ„å»ºæ—¶é—´
  dataHash: string;             // æ•°æ®å†…å®¹å“ˆå¸Œ
  changes: ChangeLog[];         // å˜æ›´æ—¥å¿—
}

interface ChangeLog {
  articleId: string;
  changeType: 'ADD' | 'UPDATE' | 'DELETE';
  description: string;
  timestamp: Date;
}
```

### ç‰ˆæœ¬è¿½è¸ª / Version Tracking

```typescript
// ç”Ÿæˆæ•°æ®ç‰ˆæœ¬ä¿¡æ¯
function generateDataVersion(): DataVersion {
  const commitHash = execSync('git rev-parse HEAD').toString().trim();
  const dataHash = calculateDataHash();
  const changes = detectChanges();
  
  return {
    version: calculateVersion(changes),
    commitHash,
    buildTime: new Date().toISOString(),
    dataHash,
    changes,
  };
}

// æ£€æµ‹æ•°æ®å˜æ›´
function detectChanges(): ChangeLog[] {
  const changes: ChangeLog[] = [];
  const lastVersion = loadLastVersion();
  
  // æ¯”è¾ƒå½“å‰æ•°æ®å’Œä¸Šä¸€ç‰ˆæœ¬
  const currentArticles = getAllArticles();
  const previousArticles = loadPreviousArticles(lastVersion);
  
  // æ£€æµ‹æ–°å¢
  for (const article of currentArticles) {
    if (!previousArticles.has(article.id)) {
      changes.push({
        articleId: article.id,
        changeType: 'ADD',
        description: `æ–°å¢æ–‡ç« : ${article.title}`,
        timestamp: new Date(),
      });
    }
  }
  
  // æ£€æµ‹æ›´æ–°
  for (const article of currentArticles) {
    const previous = previousArticles.get(article.id);
    if (previous && hasChanged(article, previous)) {
      changes.push({
        articleId: article.id,
        changeType: 'UPDATE',
        description: `æ›´æ–°æ–‡ç« : ${article.title}`,
        timestamp: new Date(),
      });
    }
  }
  
  // æ£€æµ‹åˆ é™¤
  for (const article of previousArticles.values()) {
    if (!currentArticles.has(article.id)) {
      changes.push({
        articleId: article.id,
        changeType: 'DELETE',
        description: `åˆ é™¤æ–‡ç« : ${article.title}`,
        timestamp: new Date(),
      });
    }
  }
  
  return changes;
}
```

### ç‰ˆæœ¬å›æ»š / Version Rollback

```typescript
// ç‰ˆæœ¬å›æ»š
async function rollbackToVersion(version: string): Promise<void> {
  const targetVersion = loadVersion(version);
  
  if (!targetVersion) {
    throw new Error(`ç‰ˆæœ¬ ${version} ä¸å­˜åœ¨`);
  }
  
  // å¤‡ä»½å½“å‰ç‰ˆæœ¬
  await backupCurrentVersion();
  
  // æ¢å¤ç›®æ ‡ç‰ˆæœ¬çš„æ•°æ®
  await restoreData(targetVersion.dataHash);
  
  // æ¢å¤ç´¢å¼•
  await restoreIndexes(targetVersion.commitHash);
  
  console.log(`å·²å›æ»šåˆ°ç‰ˆæœ¬ ${version}`);
}
```

## æ•°æ®è¡€ç¼˜å…³ç³»è¿½è¸ª / Data Lineage Tracking

### æ•°æ®è¡€ç¼˜å›¾ / Data Lineage Graph

```mermaid
graph LR
    A[åŸå§‹æ–‡ä»¶<br/>mainåˆ†æ”¯] --> B[OCRè¯†åˆ«<br/>ocr_cacheåˆ†æ”¯]
    B --> C[è§£ææ•°æ®<br/>parsedåˆ†æ”¯]
    C --> D[è¡¥ä¸ä¿®æ­£<br/>ocr_patchåˆ†æ”¯]
    D --> E[æ ‡å‡†åŒ–æ•°æ®<br/>parsedåˆ†æ”¯]
    E --> F[ç´¢å¼•æ„å»º<br/>indexesåˆ†æ”¯]
    E --> G[JSONæ„å»º<br/>jsonåˆ†æ”¯]
    E --> H[æ–‡æœ¬å¯¼å‡º<br/>txtåˆ†æ”¯]
    E --> I[æœç´¢ç´¢å¼•<br/>Elasticsearch]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fce4ec
```

### æ•°æ®æº¯æº / Data Provenance

```typescript
interface DataProvenance {
  articleId: string;
  source: {
    repository: string;         // èµ„æºä»“åº“
    branch: string;             // åˆ†æ”¯å
    filePath: string;           // æ–‡ä»¶è·¯å¾„
    commitHash: string;         // æäº¤å“ˆå¸Œ
  };
  transformations: {
    step: string;               // è½¬æ¢æ­¥éª¤
    timestamp: Date;            // æ‰§è¡Œæ—¶é—´
    config: any;                // é…ç½®ä¿¡æ¯
  }[];
  dependencies: {
    articleId: string;          // ä¾èµ–çš„æ–‡ç« ID
    relationship: string;       // å…³ç³»ç±»å‹
  }[];
}

// ç”Ÿæˆæ•°æ®æº¯æºä¿¡æ¯
function generateProvenance(articleId: string): DataProvenance {
  const article = getArticle(articleId);
  
  return {
    articleId,
    source: {
      repository: article.sourceRepository,
      branch: article.sourceBranch,
      filePath: article.sourceFilePath,
      commitHash: article.sourceCommitHash,
    },
    transformations: article.transformHistory,
    dependencies: findDependencies(articleId),
  };
}

// æŸ¥æ‰¾æ•°æ®ä¾èµ–
function findDependencies(articleId: string): Dependency[] {
  const dependencies: Dependency[] = [];
  const article = getArticle(articleId);
  
  // æŸ¥æ‰¾å¼•ç”¨çš„å…¶ä»–æ–‡ç« 
  for (const part of article.parts) {
    const references = extractReferences(part.text);
    for (const ref of references) {
      dependencies.push({
        articleId: ref.articleId,
        relationship: 'REFERENCES',
      });
    }
  }
  
  // æŸ¥æ‰¾ç›¸åŒæ¥æºçš„æ–‡ç« 
  const sameSource = findArticlesBySource(article.source);
  for (const related of sameSource) {
    if (related.id !== articleId) {
      dependencies.push({
        articleId: related.id,
        relationship: 'SAME_SOURCE',
      });
    }
  }
  
  return dependencies;
}
```

### å½±å“åˆ†æ / Impact Analysis

```typescript
// åˆ†ææ•°æ®å˜æ›´çš„å½±å“èŒƒå›´
function analyzeImpact(articleId: string): ImpactAnalysis {
  const article = getArticle(articleId);
  const impact: ImpactAnalysis = {
    affectedArticles: [],
    affectedIndexes: [],
    affectedSearches: [],
  };
  
  // æŸ¥æ‰¾ä¾èµ–æ­¤æ–‡ç« çš„å…¶ä»–æ–‡ç« 
  const dependents = findDependents(articleId);
  impact.affectedArticles.push(...dependents);
  
  // æŸ¥æ‰¾å—å½±å“çš„ç´¢å¼•
  const indexes = findAffectedIndexes(articleId);
  impact.affectedIndexes.push(...indexes);
  
  // æŸ¥æ‰¾å—å½±å“çš„æœç´¢
  const searches = findAffectedSearches(articleId);
  impact.affectedSearches.push(...searches);
  
  return impact;
}
```

---

**æœ€åæ›´æ–° / Last Updated**: 2025-01-XX
**ç»´æŠ¤è€… / Maintainers**: é¡¹ç›®ç»´æŠ¤å›¢é˜Ÿ
